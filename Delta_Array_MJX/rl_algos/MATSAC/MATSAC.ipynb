{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f431e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -20\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, max_agents, masked):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert model_dim % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.model_dim = model_dim\n",
    "        self.split_head_dim = model_dim // num_heads\n",
    "        self.masked = masked\n",
    "\n",
    "        self.W_Q = nn.Linear(model_dim, model_dim)\n",
    "        self.W_K = nn.Linear(model_dim, model_dim)\n",
    "        self.W_V = nn.Linear(model_dim, model_dim)\n",
    "        self.W_O = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "        if self.masked:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(max_agents, max_agents)))\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.split_head_dim)\n",
    "        if self.masked:\n",
    "            attention_scores = attention_scores.masked_fill(self.tril[:self.n_agents, :self.n_agents] == 0, float('-inf'))\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "        output = torch.matmul(attention_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        return x.view(self.bs, self.n_agents, self.num_heads, self.split_head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        return x.transpose(1, 2).contiguous().view(self.bs, self.n_agents, self.model_dim)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        self.bs, self.n_agents, _ = Q.size()\n",
    "        Q = self.split_heads(self.W_Q(Q))\n",
    "        K = self.split_heads(self.W_K(K))\n",
    "        V = self.split_heads(self.W_V(V))\n",
    "\n",
    "        attn = self.scaled_dot_product_attention(Q, K, V)\n",
    "        output = self.W_O(self.combine_heads(attn))\n",
    "        return output\n",
    "\n",
    "class FF_MLP(nn.Module):\n",
    "    def __init__(self, model_dim, dim_ff):\n",
    "        super(FF_MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(model_dim, dim_ff)\n",
    "        self.fc2 = nn.Linear(dim_ff, model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "# Position Encoder for robotics should encode the ID of the robot (i,j)\n",
    "# TODO: Change this function to capture the spatial arrangement of delta robots\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, model_dim, max_seq_len):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        pe = torch.zeros(max_seq_len, model_dim, requires_grad=False)\n",
    "        pos = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * -(np.log(10000.0) / model_dim))\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class StateEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, max_agents, dim_ff, dropout):\n",
    "        super(StateEncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(model_dim, num_heads, max_agents, masked=False)\n",
    "        self.feed_forward = FF_MLP(model_dim, dim_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm1(x)\n",
    "        attn = self.self_attention(x, x, x)\n",
    "        x = self.layer_norm2(x + self.dropout(attn))\n",
    "        ff_embed = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_embed)\n",
    "        return x\n",
    "\n",
    "class CriticDecoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, max_agents, dim_ff, dropout):\n",
    "        super(CriticDecoderLayer, self).__init__()\n",
    "        self.cross_attention = MultiHeadAttention(model_dim, num_heads, max_agents, masked=False)\n",
    "        self.feed_forward = FF_MLP(model_dim, dim_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, x, decoder_output):\n",
    "        x = self.layer_norm1(x)\n",
    "        attn = self.cross_attention(x, decoder_output, decoder_output)\n",
    "        x = self.layer_norm2(x + self.dropout(attn))\n",
    "        ff_embed = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_embed)\n",
    "        return x\n",
    "\n",
    "class ActorDecoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, max_agents, dim_ff, dropout):\n",
    "        super(ActorDecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(model_dim, num_heads, max_agents, masked=True)\n",
    "        self.cross_attention = MultiHeadAttention(model_dim, num_heads, max_agents, masked=True)\n",
    "        self.feed_forward = FF_MLP(model_dim, dim_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm3 = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, x, encoder_output):\n",
    "        x = self.layer_norm1(x)\n",
    "        attn = self.self_attention(x, x, x)\n",
    "        x = self.layer_norm2(x + self.dropout(attn))\n",
    "        attn = self.cross_attention(x, encoder_output, encoder_output)\n",
    "        x = self.layer_norm3(x + self.dropout(attn))\n",
    "        ff_embed = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_embed)\n",
    "        return x\n",
    "\n",
    "class StateEncoder(nn.Module):\n",
    "    def __init__(self, model_dim, state_dim, num_heads, max_agents, dim_ff, dropout, n_layers, pos_enc):\n",
    "        super(StateEncoder, self).__init__()\n",
    "        self.state_embedding = nn.Linear(state_dim, model_dim)\n",
    "        self.positional_encoding = pos_enc\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoder_layers = nn.ModuleList([StateEncoderLayer(model_dim, num_heads, max_agents, dim_ff, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, states):\n",
    "        \"\"\"\n",
    "        Input: states (bs, n_agents, state_dim)\n",
    "        Output: state_enc (bs, n_agents, model_dim)\n",
    "        \"\"\"\n",
    "        state_enc = self.dropout(self.positional_encoding(F.gelu(self.state_embedding(states))))\n",
    "        for layer in self.encoder_layers:\n",
    "            state_enc = layer(state_enc)\n",
    "        return state_enc\n",
    "\n",
    "class CriticDecoder(nn.Module):\n",
    "    def __init__(self, model_dim, action_dim, num_heads, max_agents, dim_ff, dropout, n_layers, pos_enc):\n",
    "        super(CriticDecoder, self).__init__()\n",
    "        self.action_embedding = nn.Linear(action_dim, model_dim)\n",
    "        self.positional_encoding = pos_enc\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([CriticDecoderLayer(model_dim, num_heads, max_agents, dim_ff, dropout) for _ in range(n_layers)])\n",
    "        self.critic_op_layer = nn.Linear(model_dim, 1)\n",
    "\n",
    "    def forward(self, state_enc, actions):\n",
    "        \"\"\"\n",
    "        Input: state_enc (bs, n_agents, model_dim)\n",
    "               actions (bs, n_agents, action_dim)\n",
    "        Output: q_value (bs, 1) --> Centralized Critic Q' = 1/N * âˆ‘Q\n",
    "        \"\"\"\n",
    "        act_enc = self.dropout(self.positional_encoding(F.gelu(self.action_embedding(actions))))\n",
    "        for layer in self.decoder_layers:\n",
    "            act_enc = layer(act_enc, state_enc)\n",
    "        q_val = self.critic_op_layer(act_enc)\n",
    "        return torch.mean(q_val, axis=1)\n",
    "\n",
    "class ActorDecoder(nn.Module):\n",
    "    def __init__(self, model_dim, action_dim, num_heads, max_agents, dim_ff, dropout, n_layers, pos_enc):\n",
    "        super(ActorDecoder, self).__init__()\n",
    "        self.action_embedding = nn.Linear(action_dim, model_dim) # Replace action embedding of Critic Decoder from this.\n",
    "        self.positional_encoding = pos_enc\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([ActorDecoderLayer(model_dim, num_heads, max_agents, dim_ff, dropout) for _ in range(n_layers)])\n",
    "        self.actor_mu_layer = nn.Linear(model_dim, action_dim)\n",
    "        self.actor_std_layer = nn.Linear(model_dim, action_dim)\n",
    "\n",
    "    def forward(self, state_enc, actions):\n",
    "        \"\"\"\n",
    "        Input: state_enc (bs, n_agents, model_dim)\n",
    "               actions (bs, n_agents, action_dim)\n",
    "        Output: decoder_output (bs, n_agents, model_dim)\n",
    "        \"\"\"\n",
    "        act_enc = self.dropout(self.positional_encoding(F.gelu(self.action_embedding(actions))))\n",
    "        for layer in self.decoder_layers:\n",
    "            act_enc = layer(act_enc, state_enc)\n",
    "        act_mean = self.actor_mu_layer(act_enc)\n",
    "        act_std = self.actor_std_layer(act_enc)\n",
    "        act_std = torch.clamp(act_std, LOG_STD_MIN, LOG_STD_MAX)\n",
    "        act_std = torch.exp(act_std)\n",
    "        return act_mean, act_std\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_limit, model_dim, num_heads, dim_ff, num_layers, dropout, delta_array_size = (8,8)):\n",
    "        super(Transformer, self).__init__()\n",
    "        \"\"\"\n",
    "        For 2D planar manipulation:\n",
    "            state_dim = 6: state, goal, pos of robot\n",
    "            action_dim = 2: action of robot\n",
    "        max_agents = delta_array_size[0] * delta_array_size[1]: Maximum number of agents in the environment\n",
    "        model_dim: size of attn layers (model_dim % num_heads = 0)\n",
    "        dim_ff: size of MLPs\n",
    "        num_layers: number of layers in encoder and decoder\n",
    "        \"\"\"\n",
    "        self.max_agents = delta_array_size[0] * delta_array_size[1]\n",
    "        self.act_limit = action_limit\n",
    "        self.action_dim = action_dim\n",
    "        self.positional_encoding = PositionalEncoder(model_dim, self.max_agents)\n",
    "\n",
    "        self.encoder = StateEncoder(model_dim, state_dim, num_heads, self.max_agents, dim_ff, dropout, num_layers['encoder'], self.positional_encoding)\n",
    "        self.decoder_critic1 = CriticDecoder(model_dim, action_dim, num_heads, self.max_agents, dim_ff, dropout, num_layers['critic'], self.positional_encoding)\n",
    "        self.decoder_critic2 = CriticDecoder(model_dim, action_dim, num_heads, self.max_agents, dim_ff, dropout, num_layers['critic'], self.positional_encoding)\n",
    "        \n",
    "        self.decoder_actor = ActorDecoder(model_dim, action_dim, num_heads, self.max_agents, dim_ff, dropout, num_layers['actor'], self.positional_encoding)\n",
    "\n",
    "    def eval_actor(self, state_enc, actions):\n",
    "        \"\"\"\n",
    "        Input: state_enc (bs, n_agents, model_dim)\n",
    "        Output: actions (bs, n_agents, action_dim)\n",
    "        \"\"\"\n",
    "        shifted_actions = torch.zeros((bs, n_agents, self.action_dim)).to(device)\n",
    "        shifted_actions[:, 1:, :] = actions[:, :-1, :]  # Input actions go from 0 to A_n-1\n",
    "        act_mean, act_std = self.decoder_actor(state_enc, shifted_actions)\n",
    "        \n",
    "        dist = torch.distributions.Normal(act_mean, act_std)\n",
    "        entropy = dist.entropy()\n",
    "\n",
    "        output_actions = dist.rsample()\n",
    "        log_probs = dist.log_prob(output_actions).sum(axis=-1)\n",
    "        log_probs -= (2*(np.log(2) - output_actions - F.softplus(-2*output_actions))).sum(axis=2)\n",
    "        \n",
    "        output_actions = torch.tanh(output_actions)\n",
    "        output_actions = self.act_limit * output_actions # Output actions go from A_0 to A_n\n",
    "        return output_actions, log_probs, entropy\n",
    "\n",
    "    def get_actions(self, state_enc, deterministic=False):\n",
    "        \"\"\" Returns actor actions, and their log probs. If deterministic=True, set action as the output of decoder. Else, sample from mean=dec output, std=exp(log_std) \"\"\"\n",
    "        bs, n_agents, _ = state_enc.size()\n",
    "        shifted_actions = torch.zeros((bs, n_agents, self.action_dim)).to(device)\n",
    "        output_actions = torch.zeros((bs, n_agents, self.action_dim)).to(device)\n",
    "        output_action_log_probs = torch.zeros((bs, n_agents)).to(device)\n",
    "\n",
    "        for i in range(n_agents):\n",
    "            act_means, act_stds = self.decoder_actor(state_enc, shifted_actions)\n",
    "\n",
    "            if deterministic:\n",
    "                output_action = self.act_limit * torch.tanh(act_means[:, i, :])\n",
    "                output_action_log_prob = 0\n",
    "            else:\n",
    "                dist = torch.distributions.Normal(act_means[:, i, :], act_stds[:, i, :])\n",
    "                output_action = dist.rsample()\n",
    "\n",
    "                output_action_log_prob = dist.log_prob(output_action).sum(axis=-1)\n",
    "                output_action_log_prob -= (2*(np.log(2) - output_action - F.softplus(-2*output_action))).sum(axis=1)\n",
    "                output_action = torch.tanh(output_action)\n",
    "                output_action = self.act_limit * output_action\n",
    "\n",
    "            output_actions[:, i, :] = output_action \n",
    "            output_action_log_probs[:, i] = output_action_log_prob\n",
    "            if (i+1) < n_agents:\n",
    "                shifted_actions[:, i+1, :] = output_action \n",
    "        return output_actions, output_action_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a6fd98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MATAC:\n",
    "    def __init__(self, env_dixt, hp_dict, logger_kwargs=dict(), train_or_test=\"train\"):\n",
    "        if train_or_test == \"train\":\n",
    "            self.logger = EpochLogger(**logger_kwargs)\n",
    "            self.logger.save_config(locals())\n",
    "\n",
    "        self.train_or_test = train_or_test\n",
    "        self.hp_dict = hp_dict\n",
    "        self.env_dict = env_dict\n",
    "        self.obs_dim = self.env_dict['observation_space']['dim']\n",
    "        self.act_dim = self.env_dict['action_space']['dim']\n",
    "        self.act_limit = self.env_dict['action_space']['high']\n",
    "\n",
    "        # n_layers_dict={'encoder': 2, 'actor': 2, 'critic': 2}\n",
    "        self.tf = Transformer(self.obs_dim, self.act_dim, self.act_limit, self.hp_dict[\"model_dim\"], self.hp_dict[\"num_heads\"], self.hp_dict[\"dim_ff\"], self.hp_dict[\"n_layers_dict\"], self.hp_dict[\"dropout\"], self.hp_dict[\"delta_array_size\"])\n",
    "        self.tf_target = deepcopy(self.tf)\n",
    "\n",
    "        self.tf = self.tf.to(device)\n",
    "        self.tf_target = self.tf_target.to(device)\n",
    "        # Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "        for p in self.tf_target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.ma_replay_buffer = MultiAgentReplayBuffer(obs_dim=self.obs_dim, act_dim=self.act_dim, size=hp_dict['replay_size'], max_agents=self.max_agents)\n",
    "\n",
    "        # Count variables (protip: try to get a feel for how different size networks behave!)\n",
    "        var_counts = tuple(core.count_vars(module) for module in [self.tf.encoder, self.tf.decoder_actor, self.tf.decoder_critic1, self.tf.decoder_critic2])\n",
    "        if self.train_or_test == \"train\":\n",
    "            self.logger.log('\\nNumber of parameters: \\t Encoder: %d, \\t Actor Decoder: %d, \\t Critic Decoder: %d\\n'%var_counts)\n",
    "\n",
    "        self.critic_params = itertools.chain(self.tf.encoder.parameters(), self.tf.decoder_critic1.parameters(), self.tf.decoder_critic2.parameters())\n",
    "        self.optimizer_critic = Adam(self.critic_params, lr=hp_dict['q_lr'])\n",
    "        self.optimizer_actor = Adam(self.tf.decoder_actor.parameters(), lr=hp_dict['pi_lr'])\n",
    "\n",
    "        # Set up model saving\n",
    "        if self.train_or_test == \"train\":\n",
    "            self.logger.setup_pytorch_saver(self.tf)\n",
    "\n",
    "    def compute_q_loss(self, s1, a, s2, r, d):\n",
    "        q1 = self.tf.decoder_critic1(s1, a)\n",
    "        q2 = self.tf.decoder_critic2(s1, a)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_state_enc = self.tf.encoder(s2)\n",
    "            next_actions, next_log_pi = self.tf.get_actions(next_state_enc)\n",
    "            \"\"\" For now our problem is a single-step problem, so we don't need to compute the next_q values. \n",
    "            TODO: Investigate if we can improve something here later. e.g. take inspiration from PPO MAT code and see if I can include entropy and shiiz to add to the q_loss\"\"\"\n",
    "            # next_q1 = self.tf_target.decoder_critic1(next_state_enc, next_actions)\n",
    "            # next_q2 = self.tf_target.decoder_critic2(next_state_enc, next_actions)\n",
    "            q_next = r # + self.hp_dict['gamma'] * (1 - d) * (torch.min(next_q1, next_q2) - self.hp_dict['alpha'] * next_log_pi)\n",
    "        \n",
    "        q_loss = F.mse_loss(q1, q_next) + F.mse_loss(q2, q_next)\n",
    "\n",
    "        q_info = dict(Q1Vals=q1.detach().numpy(), Q2Vals=q2.detach().numpy())\n",
    "        return q_loss, q_info\n",
    "\n",
    "    def compute_pi_loss(self, s1):\n",
    "        state_enc = self.tf.encoder(s1)\n",
    "        actions, log_pi = self.tf.get_actions(state_enc)\n",
    "        \n",
    "        q1_pi = self.tf.decoder_critic1(state_enc, actions)\n",
    "        q2_pi = self.tf.decoder_critic2(state_enc, actions)\n",
    "        q_pi = torch.min(q1_pi, q2_pi)\n",
    "        pi_loss = (self.hp_dict['alpha'] * log_pi - q_pi).mean()\n",
    "\n",
    "        pi_info = dict(LogPi=logp_pi.detach().numpy())\n",
    "        return pi_loss, pi_info\n",
    "\n",
    "    def update(self, batch_size):\n",
    "        data = self.ma_replay_buffer.sample_batch(batch_size)\n",
    "        states, actions, rews, new_states, dones = data['obs'].to(device), data['act'].to(device), data['rew'].to(device), data['obs2'].to(device), data['done'].to(device)\n",
    "\n",
    "        # Critic Update\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        q_loss, q_info = self.compute_q_loss(states, actions, new_states, rews, dones)\n",
    "        q_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        \n",
    "        # Set action embeddings of critic to be the same as actor so there is consistency.\n",
    "        self.tf.decoder_critic1.action_embedding.weight.data = self.tf.decoder_actor.action_embedding.weight.data.clone()\n",
    "        self.tf.decoder_critic1.action_embedding.bias.data = self.tf.decoder_actor.action_embedding.bias.data.clone()\n",
    "        self.tf.decoder_critic2.action_embedding.weight.data = self.tf.decoder_actor.action_embedding.weight.data.clone()\n",
    "        self.tf.decoder_critic2.action_embedding.bias.data = self.tf.decoder_actor.action_embedding.bias.data.clone()\n",
    "\n",
    "        logger.store(LossQ=loss_q.item(), **q_info)\n",
    "\n",
    "        # Actor Update\n",
    "        for p in self.critic_params:\n",
    "            p.requires_grad = False\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        pi_loss, pi_info = self.compute_pi_loss(states)\n",
    "        pi_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        for p in self.critic_params:\n",
    "            p.requires_grad = True\n",
    "\n",
    "        logger.store(LossPi=loss_pi.item(), **pi_info)\n",
    "\n",
    "        # Target Update\n",
    "        with torch.no_grad():\n",
    "            for p, p_target in zip(self.tf.parameters(), self.tf_target.parameters()):\n",
    "                p_target.data.mul_(self.hp_dict['tau'])\n",
    "                p_target.data.add_((1 - self.hp_dict['tau']) * p.data)\n",
    "    \n",
    "    def get_actions(self, obs, deterministic=False):\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32).to(device)\n",
    "        obs = obs.unsqueeze(0)\n",
    "        state_enc = self.tf.encoder(obs)\n",
    "        actions, _ = self.tf.get_actions(state_enc, deterministic=deterministic)\n",
    "        return actions.detach().cpu().numpy()\n",
    "        \n",
    "    def load_saved_policy(self, path='./data/rl_data/backup/matsac_expt_grasp/pyt_save/model.pt'):\n",
    "        self.tf.load_state_dict(torch.load(path, weights_only=True))\n",
    "        self.tf_target = deepcopy(self.tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49a7782a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 128])\n",
      "torch.Size([32, 10, 2]) torch.Size([32, 10])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 10, 2]) torch.Size([32, 10]) torch.Size([32, 10, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(265856, 398468, 265473, 265473)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = Transformer(state_dim=6, action_dim=2, action_limit=3, model_dim=128, num_heads=4, dim_ff=256, num_layers={'encoder': 2, 'actor': 2, 'critic': 2}, dropout=0.1, delta_array_size=(8,8))\n",
    "tf.to(device)\n",
    "\n",
    "bs = 32\n",
    "n_agents = 10\n",
    "state = torch.randn(bs, n_agents, 6).to(device)\n",
    "state_enc = tf.encoder(state)\n",
    "print(state_enc.shape)\n",
    "output_actions, op_act_log_probs = tf.get_actions(state_enc, deterministic=False)\n",
    "print(output_actions.shape, op_act_log_probs.shape)\n",
    "\n",
    "q_val = tf.decoder_critic1(state_enc, output_actions)\n",
    "print(q_val.shape)\n",
    "\n",
    "act2, log_probs, entropy = tf.eval_actor(state_enc, output_actions)\n",
    "print(act2.shape, log_probs.shape, entropy.shape)\n",
    "\n",
    "q_val2 = tf.decoder_critic2(state_enc, act2)\n",
    "output_actions.shape\n",
    "\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "tuple(count_vars(module) for module in [tf.encoder, tf.decoder_actor, tf.decoder_critic1, tf.decoder_critic2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba995d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39mtril(torch\u001b[38;5;241m.\u001b[39mones(\u001b[43mn_agent\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, n_agent \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, n_agent \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, n_agent \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_agent' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.tril(torch.ones(n_agent + 1, n_agent + 1)).view(1, 1, n_agent + 1, n_agent + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74535f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False],\n",
       "         [ True,  True, False, False, False],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 - torch.triu(torch.ones(1, 5, 5), diagonal=1)).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f2449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.9553, -1.3361, -0.5913, -0.5101],\n",
       "          [-0.5759, -0.2501,  0.4779,  1.1363],\n",
       "          [ 1.3130,  0.6983, -0.4569,  1.0888],\n",
       "          [ 0.6480, -1.3902, -1.3216, -0.5796]],\n",
       " \n",
       "         [[-1.4359, -0.1389,  1.1751,  0.4875],\n",
       "          [ 0.4797, -0.6451, -0.6324, -0.8927],\n",
       "          [ 1.6020, -1.0250,  2.5324,  0.8271],\n",
       "          [-0.0426,  0.6527,  0.7068, -0.5352]]]),\n",
       " tensor([[[ 0.0000,  0.0000,  0.0000,  9.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  9.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  9.0000],\n",
       "          [-0.9553, -1.3361, -0.5913,  9.0000],\n",
       "          [-0.5759, -0.2501,  0.4779,  9.0000],\n",
       "          [ 1.3130,  0.6983, -0.4569,  9.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  9.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  9.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  9.0000],\n",
       "          [-1.4359, -0.1389,  1.1751,  9.0000],\n",
       "          [ 0.4797, -0.6451, -0.6324,  9.0000],\n",
       "          [ 1.6020, -1.0250,  2.5324,  9.0000]]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc154ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
