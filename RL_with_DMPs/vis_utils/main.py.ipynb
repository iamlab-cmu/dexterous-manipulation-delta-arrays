{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fcf85cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPS attempt 1 of 5: \n",
      "policy_params_mean_init_this_attempt:\n",
      "[0.66 0.2  0.   0.4 ]\n",
      "policy_params_var_init_this_attempt:\n",
      "[[0.15]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "mean and cov must have same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31730/2154292829.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m                                         \u001b[0mmax_num_reps_attempts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_num_reps_attempts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                                         \u001b[0mdebug_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                                         )\n",
      "\u001b[0;32m~/Sarvesh/DeltaZ/Python/RL_SBP/env_interaction.py\u001b[0m in \u001b[0;36msolve_env_using_reps\u001b[0;34m(env, policy, policy_params_mean_init, policy_params_var_init, num_policy_rollouts_before_reps_update, max_reps_param_updates, env_convergence_criteria, reps_hyperparams, max_num_reps_attempts, enable_sum_rewards_over_rollout, debug_info, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# Sample policy parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 policy_params = np.random.multivariate_normal(mean=policy_params_mean, \\\n\u001b[0;32m--> 232\u001b[0;31m                                                             cov=policy_params_var)\n\u001b[0m\u001b[1;32m    233\u001b[0m                 \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mean and cov must have same length"
     ]
    }
   ],
   "source": [
    "import potentiometer_simpler\n",
    "import control_delta\n",
    "import env_interaction\n",
    "from identity_policy import IdentityLowLevelPolicy\n",
    "import numpy as np\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    env = potentiometer_simpler.Potentiometer()\n",
    "    env.set_resistance(5000, 10000)\n",
    "\n",
    "    pol = IdentityLowLevelPolicy(env._dim_action) # this will be dim 2 in your case\n",
    "    reps_init_mode = \"uninformed\"\n",
    "    if reps_init_mode == \"informed\":\n",
    "        max_num_reps_attempts = 2\n",
    "        max_reps_param_updates = 12\n",
    "    else:\n",
    "        \"\"\" Does no of episodes have effect on convergence?? \"\"\"\n",
    "        max_num_reps_attempts = 5\n",
    "        max_reps_param_updates = 20\n",
    "\n",
    "    num_policy_rollouts_before_reps_update = 10 * pol.num_params()\n",
    "    env_convergence_criteria = {\"env_solved\": 0.9}\n",
    "\n",
    "    if reps_init_mode == \"informed\":\n",
    "        policy_params_mean_init = env.controller_gt_params\n",
    "        policy_params_var_init = np.eye(pol.num_params()) * 0.01\n",
    "    else:\n",
    "        policy_params_mean_init = np.zeros(pol.num_params())+0.3\n",
    "        \n",
    "        \"\"\" TODO: Change std dev and see effect on exploration\"\"\"\n",
    "        policy_params_var_init = np.eye(pol.num_params()) * 0.15\n",
    "\n",
    "        reps_converged, low_level_policy_params_mean, \\\n",
    "            low_level_policy_params_var, solve_env_info = \\\n",
    "                    env_interaction.solve_env_using_reps(env,\n",
    "                                        pol,   # this is the pol variable above\n",
    "                                        policy_params_mean_init,\n",
    "                                        policy_params_var_init,\n",
    "                                        num_policy_rollouts_before_reps_update,\n",
    "                                        max_reps_param_updates,\n",
    "                                        env_convergence_criteria,\n",
    "                                        max_num_reps_attempts=max_num_reps_attempts,\n",
    "                                        debug_info=True,\n",
    "                                        verbose=True,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(4)+np.array([0.66, 0.2, 0, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cda02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pouipoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e26b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c85fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "reps_policy = {'reps_converged': reps_converged,\n",
    "              'low_level_policy_params_mean': low_level_policy_params_mean,\n",
    "              'low_level_policy_params_var': low_level_policy_params_var,\n",
    "              'solve_env_info': solve_env_info}\n",
    "\n",
    "with open('REPS_1_theta_policy.pickle', 'wb') as f:\n",
    "    pickle.dump(reps_policy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Expt_0/REPS_1_theta_policy.pickle', 'rb') as handle:\n",
    "    reps_policy_loaded = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2395f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_utils.analysis import reps_solve_info_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfead7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5aa76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78371c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b713452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "verbose = True\n",
    "path_to_reps_info = 'REPS_1_theta_policy.pickle'\n",
    "\n",
    "reps_converged = solve_env_info[\"reps_converged\"]\n",
    "policy_params_mean = solve_env_info[\"policy_params_mean\"]\n",
    "mean_param_hist = solve_env_info[\"history\"][\"policy_params_mean\"]\n",
    "var_diag_param_hist = solve_env_info[\"history\"][\"policy_params_var_diag\"]\n",
    "mean_rew_hist = solve_env_info[\"history\"][\"mean_reward\"]\n",
    "\n",
    "num_params = len(solve_env_info[\"policy_params_mean\"])\n",
    "num_reps_attempts = solve_env_info[\"num_reps_attempts\"]\n",
    "\n",
    "# this might be a ragged array, so we flatten it\n",
    "assert len(mean_rew_hist) == num_reps_attempts\n",
    "mean_rew_hist_all_attempts = np.hstack(\n",
    "    [mean_rew_hist[a] for a in range(num_reps_attempts)]\n",
    ")\n",
    "iter_param_updates = range(len(mean_rew_hist_all_attempts))\n",
    "\n",
    "if verbose:\n",
    "    print(f'REPS solve info for \"{path_to_reps_info}\":')\n",
    "    print(f\" -> Solved: {reps_converged}\")\n",
    "    print(f\" -> Parameters (mean): {policy_params_mean}\")\n",
    "\n",
    "num_subplots = num_params + 1\n",
    "fig, ax = plt.subplots(num_subplots, 1, sharex=True)\n",
    "prop_cycle = plt.rcParams[\"axes.prop_cycle\"]\n",
    "colors = cycle(prop_cycle.by_key()[\"color\"])\n",
    "for p in range(num_subplots):\n",
    "\n",
    "    if p == 0:\n",
    "        # show reward\n",
    "        ax[p].plot(\n",
    "            iter_param_updates,\n",
    "            mean_rew_hist_all_attempts,\n",
    "            \".-\",\n",
    "            color=next(colors),\n",
    "        )\n",
    "        ax[p].grid()\n",
    "        ax[p].set_ylabel(f\"Reward\")\n",
    "\n",
    "    else:\n",
    "        idx_p = p - 1\n",
    "        # this might be a ragged array, so we flatten it\n",
    "        assert len(mean_param_hist) == num_reps_attempts\n",
    "        mean_param_hist_all_attempts = np.hstack(\n",
    "            [\n",
    "                np.array(mean_param_hist[a])[:, idx_p]\n",
    "                for a in range(num_reps_attempts)\n",
    "            ]\n",
    "        )\n",
    "        # assert len(mean_param_hist_all_attempts) == (num_reps_param_updates + 1)\n",
    "\n",
    "        assert len(var_diag_param_hist) == num_reps_attempts\n",
    "        var_diag_param_hist_all_attempts = np.hstack(\n",
    "            [\n",
    "                np.array(var_diag_param_hist[a])[:, idx_p]\n",
    "                for a in range(num_reps_attempts)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        stdev_diag_param_hist_all_attempts = np.sqrt(\n",
    "            var_diag_param_hist_all_attempts\n",
    "        )\n",
    "\n",
    "        assert len(mean_param_hist_all_attempts) == len(\n",
    "            stdev_diag_param_hist_all_attempts\n",
    "        )\n",
    "        assert len(mean_param_hist_all_attempts) == len(iter_param_updates)\n",
    "\n",
    "        mean_p_stdev = (\n",
    "            mean_param_hist_all_attempts + stdev_diag_param_hist_all_attempts\n",
    "        )\n",
    "        mean_m_stdev = (\n",
    "            mean_param_hist_all_attempts - stdev_diag_param_hist_all_attempts\n",
    "        )\n",
    "\n",
    "        this_color = next(colors)\n",
    "\n",
    "        ax[p].plot(\n",
    "            iter_param_updates,\n",
    "            mean_param_hist_all_attempts,\n",
    "            \".-\",\n",
    "            color=this_color,\n",
    "        )\n",
    "        ax[p].fill_between(\n",
    "            iter_param_updates,\n",
    "            mean_p_stdev,\n",
    "            mean_m_stdev,\n",
    "            alpha=0.25,\n",
    "            color=this_color,\n",
    "        )\n",
    "        ax[p].grid()\n",
    "        ax[p].set_ylabel(f\"Parameter {idx_p}\")\n",
    "\n",
    "    if p == (num_subplots - 1):\n",
    "        ax[p].set_xlabel(\"parameter update iteration\")\n",
    "\n",
    "plt.xlim((iter_param_updates[0], iter_param_updates[-1]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_param_hist = solve_env_info[\"history\"][\"policy_params_mean\"]\n",
    "var_diag_param_hist = solve_env_info[\"history\"][\"policy_params_var_diag\"]\n",
    "mean_param_hist, var_diag_param_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf036e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "x_axis = np.arange(-1, 1, 0.001)\n",
    "\n",
    "for m,v in zip(mean_param_hist[0], var_diag_param_hist[0]):\n",
    "    norm_vals = np.array(norm.pdf(x_axis,m,v))\n",
    "    plt.plot((x_axis+1)*180, (norm_vals+1)*180)\n",
    "\n",
    "plt.ylim(0,20000)\n",
    "plt.xlim(180,360)\n",
    "plt.title(f\"Ideal Resistance Value for Theta = 312\\nObtained Mean Resistance Value for Theta: {(m[0]+1)*180}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdfa764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m,v = mean_param_hist[0][0], var_diag_param_hist[0][0]\n",
    "norm_vals = np.array(norm.pdf(x_axis,m,v))\n",
    "plt.plot((x_axis+1)*180, (norm_vals+1)*180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bea624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
